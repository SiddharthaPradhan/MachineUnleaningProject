{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB0iUjarpgrE",
        "outputId": "1885eb4e-7c74-42fd-b8e7-7ec97c7bec34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device: cuda\n",
            "2.1.0+cu118\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.models import resnet18\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Running on device:\", DEVICE)\n",
        "\n",
        "# manual random seed is used for dataset partitioning\n",
        "# to ensure reproducible results across runs\n",
        "RNG = torch.Generator().manual_seed(42)\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIHgsQrV3CrN",
        "outputId": "e3c7b519-2085-4a4e-d81d-926ac36eb941"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec  6 06:29:33 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P0    29W /  70W |   1835MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ],
      "source": [
        "# check if cuda is installed\n",
        "!nvidia-smi\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6saW2xkgqXHI",
        "outputId": "a2c1d15d-814c-414f-ec5e-e1782e7ca6b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# some of this portion is from (i.e starting point): https://github.com/unlearning-challenge/starting-kit/blob/main/unlearning-CIFAR10.ipynb\n",
        "# loading the normalization applied during training\n",
        "\n",
        "normalize = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(), # mean and sd from pytorch\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# download train set\n",
        "train_set = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=normalize\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=256, shuffle=True, num_workers=2)\n",
        "\n",
        "# download held out data into test set\n",
        "held_out = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=normalize\n",
        ")\n",
        "\n",
        "\n",
        "test_loader = DataLoader(held_out, batch_size=256, shuffle=False, num_workers=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "A06aXknE1T4k"
      },
      "outputs": [],
      "source": [
        "def accuracy(nn, dataLoader):\n",
        "    nn.eval()\n",
        "    num_correct = 0\n",
        "    total = 0\n",
        "    for inputs, targets in dataLoader:\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "        outputs = nn(inputs) # get logits\n",
        "        _, predicted = outputs.max(1) # select max index\n",
        "        total += targets.size(0)\n",
        "        num_correct += predicted.eq(targets).sum().item() # sum correct instances\n",
        "    return num_correct / total\n",
        "\n",
        "def accuracy_from_output(logits, truthLabels):\n",
        "    out = torch.argmax(logits.detach(), dim=1)\n",
        "    return (truthLabels==out).sum().item()\n",
        "\n",
        "def evaluate(nn, dataLoader):\n",
        "    nn.eval()\n",
        "    total_correct = 0.0\n",
        "    total_loss = 0.0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataLoader:\n",
        "              inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "              outputs = nn(inputs) # get logits\n",
        "              loss = F.cross_entropy(outputs, targets)\n",
        "              total += targets.size(0)\n",
        "              total_correct += accuracy_from_output(outputs, targets)\n",
        "              total_loss += loss.detach()\n",
        "    return total_correct/total, total_loss/total\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swRAdDvq0a5H",
        "outputId": "d5e33869-bd3d-4c77-9e5b-7422d86c483a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 99.46199999999999%\n",
            "Test accuracy: 88.64%\n"
          ]
        }
      ],
      "source": [
        "# download pre-trained weights, TODO: plug in trained weights from Nur\n",
        "# using unlearning-challenge weights for now\n",
        "\n",
        "local_path = \"weights_resnet18_cifar10.pth\"\n",
        "if not os.path.exists(local_path):\n",
        "    response = requests.get(\n",
        "        \"https://storage.googleapis.com/unlearning-challenge/weights_resnet18_cifar10.pth\"\n",
        "    )\n",
        "    open(local_path, \"wb\").write(response.content)\n",
        "\n",
        "pretrained_state_dict = torch.load(local_path, map_location=DEVICE)\n",
        "\n",
        "# load model with pre-trained weights\n",
        "model = resnet18(weights=None, num_classes=10)\n",
        "model.load_state_dict(pretrained_state_dict)\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "print(f\"Train accuracy: {100.0 * accuracy(model, train_loader)}%\")\n",
        "print(f\"Test accuracy: {100.0 * accuracy(model, test_loader)}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gQnE0p-A8CRB"
      },
      "outputs": [],
      "source": [
        "# Noise data\n",
        "class Noise(nn.Module):\n",
        "    def __init__(self, *dim):\n",
        "        super().__init__()\n",
        "        self.noise = torch.nn.Parameter(torch.randn(*dim), requires_grad = True)\n",
        "\n",
        "    def forward(self):\n",
        "        return self.noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6HrMXS6oT7vn"
      },
      "outputs": [],
      "source": [
        "def fast_effective_unlearning(net, classesToForget, retainSamples):\n",
        "    '''\n",
        "    net: NN.module (i.e the neural network)\n",
        "    classesToForget: List (i.e the list of classes to unlearn)\n",
        "    retainSamples: List (i.e the (images, label_idx) sampled from D_retain)\n",
        "    '''\n",
        "    # Learn noise\n",
        "    BATCH_SIZE = 256 # same as paper\n",
        "    noises = {} # noise dict --> maps foget class to learnt noises (i.e Noise nn.module)\n",
        "    IMG_SIZE = (3, 32, 32)\n",
        "    L2_REG = 0.1 # same as in the paper\n",
        "\n",
        "    print(\"Phase 1: Learning Noise for forget classes\")\n",
        "\n",
        "    net.eval() # freeze weights when generating noise\n",
        "    for classF in classesToForget:\n",
        "        print(f\"Learning noise matrices for class = {classF}\")\n",
        "        noises[classF] = Noise(BATCH_SIZE, *IMG_SIZE).to(DEVICE)\n",
        "        opt = torch.optim.Adam(noises[classF].parameters(), lr = 0.1, weight_decay=L2_REG) # same learning rate in the paper\n",
        "        noises[classF].train(True)\n",
        "        numEpochs = 5 # same as paper\n",
        "        stepPerEpoch = 20 # same as in paper\n",
        "        for epoch in range(numEpochs):\n",
        "            total_loss = []\n",
        "            for batch in range(stepPerEpoch):\n",
        "                inputs = noises[classF]() # input set as noise matrix\n",
        "                labels = torch.zeros(BATCH_SIZE).to(DEVICE) + classF # set all labels as class to forget\n",
        "                labels = labels.long()\n",
        "                outputs = net(inputs) # get outputs from trained nn\n",
        "                loss = -F.cross_entropy(outputs, labels)\n",
        "                opt.zero_grad()\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "                total_loss.append(loss.cpu().detach().numpy())\n",
        "        print(\"Loss: {}\".format(np.mean(total_loss)))\n",
        "\n",
        "    print(\"Phase 1 complete.\")\n",
        "    print(\"Forget Set Performance:\", evaluate(net, forgetSetTestLoader))\n",
        "    print(\"Retain Set Performance:\", evaluate(net, retainSetTestLoader))\n",
        "    print(\"+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\")\n",
        "\n",
        "\n",
        "    print(\"Phase 2: Impair\")\n",
        "    noiseData = []\n",
        "    numBatches = 20 # number of times the noisy data is replicated, same as in paper\n",
        "\n",
        "    for classF in classesToForget:\n",
        "        for i in range(numBatches):\n",
        "            batch = noises[classF]().cpu().detach()\n",
        "            for i in range(batch[0].size(0)): # for each noise matrix in batch\n",
        "                noiseData.append((batch[i], torch.tensor(classF))) # (noise matrix, class_num)\n",
        "    # TODO: what if we randomized the labels for classF as well?\n",
        "\n",
        "    retainSampleImpair = []\n",
        "    for i in range(len(retainSamples)):\n",
        "        retainSampleImpair.append((retainSamples[i][0].cpu(), torch.tensor(retainSamples[i][1]))) # data, label\n",
        "\n",
        "    impairData = []\n",
        "    impairData.extend(noiseData)\n",
        "    impairData.extend(retainSampleImpair)\n",
        "    impairLoader = torch.utils.data.DataLoader(impairData, batch_size=256, shuffle = True)\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr = 0.02)\n",
        "\n",
        "    net.train() # set to training mode\n",
        "    NUM_EPOCH_IMPAIR = 1 # same as in the paper\n",
        "    for epoch in range(NUM_EPOCH_IMPAIR):\n",
        "        totalAcc = 0.0\n",
        "        totalLoss = 0.0\n",
        "        for impairData in impairLoader:\n",
        "            inputs, labels = impairData\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.clone().detach().to(DEVICE)\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            loss = F.cross_entropy(outputs, labels) # cross entropy loss\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            totalLoss += loss.item()\n",
        "            totalAcc += accuracy_from_output(outputs, labels)\n",
        "        print(f\"Epoch {epoch+1} | Train loss: {totalLoss/len(impairLoader.dataset)}, Train Acc:{totalAcc*100/len(impairLoader.dataset)}%\")\n",
        "\n",
        "\n",
        "    print(\"Phase 2 complete.\")\n",
        "    print(\"Forget Set Performance:\", evaluate(net, forgetSetTestLoader))\n",
        "    print(\"Retain Set Performance:\", evaluate(net, retainSetTestLoader))\n",
        "    print(\"+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\")\n",
        "\n",
        "\n",
        "    print(\"Phase 3: Repair\")\n",
        "\n",
        "    repairLoader = torch.utils.data.DataLoader(retainSampleImpair, batch_size=256, shuffle = True)\n",
        "    repairOtpm = torch.optim.Adam(net.parameters(), lr = 0.01)\n",
        "\n",
        "    NUM_EPOCH_REPAIR = 20\n",
        "    net.train()\n",
        "    for epoch in range(NUM_EPOCH_REPAIR):\n",
        "        total_loss = 0.0\n",
        "        total_acc = 0.0\n",
        "        for data in repairLoader:\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.clone().detach().to(DEVICE)\n",
        "            outputs = net(inputs)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "            repairOtpm.zero_grad()\n",
        "            loss.backward()\n",
        "            repairOtpm.step()\n",
        "            total_loss += loss.item()\n",
        "            total_acc += accuracy_from_output(outputs, labels)\n",
        "        print(f\"Epoch {epoch+1} | Train loss: {total_loss/len(repairLoader.dataset)}, Training Acc:{total_acc*100/len(repairLoader.dataset)}%\")\n",
        "\n",
        "    print(\"Phase 3 complete.\")\n",
        "    print(\"+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\")\n",
        "    net.eval()\n",
        "    return net\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_UxQXaMKiQo_"
      },
      "outputs": [],
      "source": [
        "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "BATCH_SIZE = 256\n",
        "forget_classes = [0, 6]\n",
        "num_classes = len(classes)\n",
        "\n",
        "train_set_classes = {} # define dict: class -> class imgs\n",
        "for i in classes:\n",
        "    train_set_classes[i] = []\n",
        "for img, label in train_set:\n",
        "    train_set_classes[label].append((img, label))\n",
        "\n",
        "test_set_classes = {}\n",
        "for i in classes:\n",
        "    test_set_classes[i] = []\n",
        "for img, label in held_out:\n",
        "    test_set_classes[label].append((img, label))\n",
        "\n",
        "# number of retain samples from each class, needed for repair and impair step\n",
        "# subset of D_retain\n",
        "numRetainSamples = 1000\n",
        "retainedSamples = []\n",
        "for i in classes:\n",
        "    if classes[i] not in forget_classes:\n",
        "        # get first numRetainSamples from each class not in the forget set\n",
        "        retainedSamples.extend(train_set_classes[i][:numRetainSamples])\n",
        "\n",
        "# retain test set\n",
        "retainTestSet = []\n",
        "for classR in classes:\n",
        "    if classR not in forget_classes:\n",
        "        for img, label in test_set_classes[classR]:\n",
        "            retainTestSet.append((img, label))\n",
        "\n",
        "# forget test set\n",
        "forgetTestSet = []\n",
        "for classF in classes:\n",
        "    if classF in forget_classes:\n",
        "        for img, label in test_set_classes[classF]:\n",
        "            forgetTestSet.append((img, label))\n",
        "\n",
        "forgetSetTestLoader = DataLoader(forgetTestSet, BATCH_SIZE, num_workers=2)\n",
        "retainSetTestLoader = DataLoader(retainTestSet, BATCH_SIZE, num_workers=2)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-FIltIomDWx",
        "outputId": "b2aea266-8293-4018-e14e-0b65a5e0e4ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 5.72 µs\n",
            "Phase 1: Learning Noise for forget classes\n",
            "Learning noise matrices for class = 0\n",
            "Loss: -4.171452045440674\n",
            "Learning noise matrices for class = 6\n",
            "Loss: -6.774816989898682\n",
            "Phase 1 complete.\n",
            "Forget Set Performance: (0.916, tensor(0.0013, device='cuda:0'))\n",
            "Retain Set Performance: (0.879, tensor(0.0019, device='cuda:0'))\n",
            "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
            "Phase 2: Impair\n",
            "Epoch 1 | Train loss: 0.00753837710824506, Train Acc:29.704433497536947%\n",
            "Phase 2 complete.\n",
            "Forget Set Performance: (0.003, tensor(0.0908, device='cuda:0'))\n",
            "Retain Set Performance: (0.256875, tensor(0.0138, device='cuda:0'))\n",
            "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
            "Phase 3: Repair\n",
            "Epoch 1 | Train loss: 0.005908283486962319, Training Acc:41.95%\n",
            "Epoch 2 | Train loss: 0.0051711806654930114, Training Acc:49.375%\n",
            "Epoch 3 | Train loss: 0.004602989196777344, Training Acc:55.475%\n",
            "Epoch 4 | Train loss: 0.004178617484867573, Training Acc:60.05%\n",
            "Epoch 5 | Train loss: 0.0037429906353354456, Training Acc:64.1375%\n",
            "Epoch 6 | Train loss: 0.0034547781497240066, Training Acc:67.8125%\n",
            "Epoch 7 | Train loss: 0.0029599050953984262, Training Acc:72.75%\n",
            "Epoch 8 | Train loss: 0.002558722667396069, Training Acc:77.325%\n",
            "Epoch 9 | Train loss: 0.0021836078390479086, Training Acc:80.0875%\n",
            "Epoch 10 | Train loss: 0.0018662669099867343, Training Acc:83.2125%\n",
            "Epoch 11 | Train loss: 0.001525020495057106, Training Acc:86.2%\n",
            "Epoch 12 | Train loss: 0.001199997927993536, Training Acc:89.4125%\n",
            "Epoch 13 | Train loss: 0.001068095127120614, Training Acc:90.85%\n",
            "Epoch 14 | Train loss: 0.0010652660485357046, Training Acc:90.6875%\n",
            "Epoch 15 | Train loss: 0.0006672071078792215, Training Acc:94.1375%\n",
            "Epoch 16 | Train loss: 0.0005435220678336918, Training Acc:95.2375%\n",
            "Epoch 17 | Train loss: 0.0004601555694825947, Training Acc:95.9625%\n",
            "Epoch 18 | Train loss: 0.00038328745821490884, Training Acc:96.9875%\n",
            "Epoch 19 | Train loss: 0.0005085954298265278, Training Acc:95.5375%\n",
            "Epoch 20 | Train loss: 0.0004361397651955485, Training Acc:96.0375%\n",
            "Phase 3 complete.\n",
            "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
            "Unlearned model performance metrics on Forget Class:\n",
            "Accuracy: 0.0\n",
            "Loss: 0.0976337417960167\n",
            "Unlearned model performance metrics on Retain Class:\n",
            "Accuracy: 0.61425\n",
            "Loss: 0.007896616123616695\n"
          ]
        }
      ],
      "source": [
        "%time\n",
        "# load model with pre-trained weights for unlearning\n",
        "model_for_unlearning = resnet18(weights=None, num_classes=10)\n",
        "model_for_unlearning.load_state_dict(pretrained_state_dict)\n",
        "model_for_unlearning.to(DEVICE)\n",
        "\n",
        "# perform unlearning\n",
        "unlearned_model = fast_effective_unlearning(model_for_unlearning, forget_classes, retainedSamples)\n",
        "\n",
        "print(\"Unlearned model performance metrics on Forget Class:\")\n",
        "acc, loss = evaluate(unlearned_model, forgetSetTestLoader)\n",
        "print(f\"Accuracy: {acc}\")\n",
        "print(f\"Loss: {loss}\")\n",
        "\n",
        "\n",
        "print(\"Unlearned model performance metrics on Retain Class:\")\n",
        "acc2, loss2 = evaluate(unlearned_model, retainSetTestLoader)\n",
        "print(f\"Accuracy: {acc2}\")\n",
        "print(f\"Loss: {loss2}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"weights_resnet18_cifar10_unlearned_sid.pth\")"
      ],
      "metadata": {
        "id": "yAiS5Lcjr6Yj"
      },
      "execution_count": 24,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}