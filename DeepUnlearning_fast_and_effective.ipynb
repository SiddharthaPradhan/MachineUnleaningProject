{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB0iUjarpgrE",
        "outputId": "a2831adc-6f8f-4ca8-ddb0-08c45c39acba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.models import resnet18\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Running on device:\", DEVICE)\n",
        "\n",
        "# manual random seed is used for dataset partitioning\n",
        "# to ensure reproducible results across runs\n",
        "RNG = torch.Generator().manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if cuda is installed\n",
        "!nvidia-smi\n",
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIHgsQrV3CrN",
        "outputId": "b78f3563-a6c0-4557-bcfd-4c9af428ccb7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Dec  5 21:24:30 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P8    11W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# some of this portion is from (i.e starting point): https://github.com/unlearning-challenge/starting-kit/blob/main/unlearning-CIFAR10.ipynb\n",
        "# loading the normalization applied during training\n",
        "\n",
        "normalize = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# download train set\n",
        "train_set = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=normalize\n",
        ")\n",
        "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "# download held out data into test set\n",
        "held_out = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=normalize\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(held_out, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "# download the forget and retain index split, will be replaced with the specific class\n",
        "local_path = \"forget_idx.npy\"\n",
        "if not os.path.exists(local_path):\n",
        "    response = requests.get(\n",
        "        \"https://storage.googleapis.com/unlearning-challenge/\" + local_path\n",
        "    )\n",
        "    open(local_path, \"wb\").write(response.content)\n",
        "forget_idx = np.load(local_path)\n",
        "\n",
        "# construct indices to retain based on the forget set\n",
        "forget_mask = np.zeros(len(train_set.targets), dtype=bool)\n",
        "forget_mask[forget_idx] = True\n",
        "retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
        "\n",
        "# split train set into a forget and a retain set\n",
        "forget_set = torch.utils.data.Subset(train_set, forget_idx)\n",
        "retain_set = torch.utils.data.Subset(train_set, retain_idx)\n",
        "\n",
        "forget_loader = torch.utils.data.DataLoader(\n",
        "    forget_set, batch_size=128, shuffle=True, num_workers=2\n",
        ")\n",
        "retain_loader = torch.utils.data.DataLoader(\n",
        "    retain_set, batch_size=128, shuffle=True, num_workers=2, generator=RNG\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6saW2xkgqXHI",
        "outputId": "35421b02-ea52-45ca-8796-27cae2ef4435"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 43388491.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download pre-trained weights, TODO: plug in trained weights from Nur\n",
        "# using unlearning-challenge weights for now\n",
        "\n",
        "local_path = \"weights_resnet18_cifar10.pth\"\n",
        "if not os.path.exists(local_path):\n",
        "    response = requests.get(\n",
        "        \"https://storage.googleapis.com/unlearning-challenge/weights_resnet18_cifar10.pth\"\n",
        "    )\n",
        "    open(local_path, \"wb\").write(response.content)\n",
        "\n",
        "pretrained_state_dict = torch.load(local_path, map_location=DEVICE)\n",
        "\n",
        "# load model with pre-trained weights\n",
        "model = resnet18(weights=None, num_classes=10)\n",
        "model.load_state_dict(pretrained_state_dict)\n",
        "model.to(DEVICE)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swRAdDvq0a5H",
        "outputId": "e782bdeb-8819-4439-cbd3-3a4cc9b833f5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(nn, dataLoader):\n",
        "    num_correct = 0\n",
        "    total = 0\n",
        "    for inputs, targets in dataLoader:\n",
        "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "        outputs = nn(inputs) # get logits\n",
        "        _, predicted = outputs.max(1) # select max index\n",
        "        total += targets.size(0)\n",
        "        num_correct += predicted.eq(targets).sum().item() # sum correct instances\n",
        "    return num_correct / total\n",
        "\n",
        "\n",
        "print(f\"Train accuracy: {100.0 * accuracy(model, train_loader)}%\")\n",
        "print(f\"Test accuracy: {100.0 * accuracy(model, test_loader)}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A06aXknE1T4k",
        "outputId": "748ab4f7-6d5a-4b52-e335-13ce803b2a22"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 99.46199999999999%\n",
            "Test accuracy: 88.34%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Noise data\n",
        "class Noise(nn.Module):\n",
        "    def __init__(self, *dim):\n",
        "        super().__init__()\n",
        "        self.noise = torch.nn.Parameter(torch.randn(*dim), requires_grad = True)\n",
        "\n",
        "    def forward(self):\n",
        "        return self.noise"
      ],
      "metadata": {
        "id": "gQnE0p-A8CRB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "forget_classes = [1, 3 ,9]\n",
        "\n",
        "num_classes = len(classes)\n",
        "\n",
        "train_set_classes = {} # define dict: class -> class data\n",
        "for i in range(num_classes):\n",
        "    train_set_classes[i] = []\n",
        "for img, label in train_set:\n",
        "    train_set_classes[label].append((img, label))\n",
        "\n",
        "test_set_classes = {}\n",
        "for i in range(num_classes):\n",
        "    test_set_classes[i] = []\n",
        "for img, label in held_out:\n",
        "    test_set_classes[label].append((img, label))\n",
        "\n",
        "# number of retain samples from each class, needed for repair and impair step\n",
        "numRetainSamples = 1000\n",
        "retainedSamples = []\n",
        "for i in range(num_classes):\n",
        "    if classes[i] not in forget_classes:\n",
        "        # get first numRetainSamples from each class not in the forget set\n",
        "        retainedSamples.append(train_set_classes[i][:numRetainSamples])"
      ],
      "metadata": {
        "id": "DCVr4KjE3x8R"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fast_effective_unlearning(net, classesToForget, retainSamples):\n",
        "    '''\n",
        "    net: NN.module (i.e the neural network)\n",
        "    classesToForget: List (i.e the list of classes to unlearn)\n",
        "    retainSamples: List (i.e the images sampled from D_retain)\n",
        "    '''\n",
        "    # Learn noise\n",
        "    BATCH_SIZE = 256 # same as paper\n",
        "    noises = {} # noise dict --> maps foget class to learnt noises (i.e Noise nn.module)\n",
        "    IMG_SIZE = (3, 32, 32)\n",
        "    L2_REG = 0.1 # same as in the paper\n",
        "\n",
        "    print(\"Phase 1: Learning Noise for forget classes\")\n",
        "\n",
        "    for classF in classesToForget:\n",
        "        print(f\"Learning noise matrices for class= {classF}\")\n",
        "        noises[classF] = Noise(BATCH_SIZE, *IMG_SIZE).to(DEVICE)\n",
        "        opt = torch.optim.Adam(noises[classF].parameters(), lr = 0.1, weight_decay=L2_REG) # same learning rate in the paper\n",
        "\n",
        "        numEpochs = 5\n",
        "        stepPerEpoch = 5\n",
        "        for epoch in range(numEpochs):\n",
        "            total_loss = []\n",
        "            for batch in range(stepPerEpoch):\n",
        "                inputs = noises[classF]() # input set as noise matrix\n",
        "                labels = torch.zeros(BATCH_SIZE).to(DEVICE) + classF # set all labels as class to forget\n",
        "                labels = labels.long()\n",
        "                outputs = net(inputs) # get outputs from trained nn\n",
        "                loss = -F.cross_entropy(outputs, labels) # L2 regularization performed by optimizer opt\n",
        "                opt.zero_grad()\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "                total_loss.append(loss.cpu().detach().numpy())\n",
        "        print(\"Loss: {}\".format(np.mean(total_loss)))\n",
        "\n",
        "    print(\"Phase 1 complete.\")\n",
        "\n",
        "    print(\"Phase 2: Impair\")\n",
        "\n",
        "    noiseData = []\n",
        "    numBatches = 20 # number of times the noisy data is replicated, same as in paper\n",
        "    class_num = 0\n",
        "\n",
        "    for classF in classesToForget:\n",
        "        for i in range(numBatches):\n",
        "            batch = noises[classF]().cpu().detach()\n",
        "            for i in range(batch[0].size(0)): # for each noise matrix in batch\n",
        "                noiseData.append((batch[i], torch.tensor(classF))) # (noise matrix, class_num)\n",
        "    # TODO: what if we randomized the labels for classF as well?\n",
        "\n",
        "    retainSampleImpair = []\n",
        "    for i in range(len(retainSamples)):\n",
        "        retainSampleImpair.append((retainSamples[i][0].cpu(), torch.tensor(retainSamples[i][1]))) # data, label\n",
        "\n",
        "    impairData = []\n",
        "    impairData.append(noiseData, retainSamretainSampleImpairpleCpu)\n",
        "    impairLoader = torch.utils.data.DataLoader(impairData, batch_size=256, shuffle = True)\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.02)\n",
        "\n",
        "    NUM_EPOCH_IMPAIR = 1 # same as in the paper\n",
        "    for epoch in range(NUM_EPOCH_IMPAIR):\n",
        "        net.train(True) # set to training mode\n",
        "        totalAcc = 0.0\n",
        "        totalLoss = 0.0\n",
        "        for impairData in impairLoader:\n",
        "            inputs, labels = impairData\n",
        "            inputs = inputs.cuda()\n",
        "            labels = torch.tensor(labels).cuda()\n",
        "            outputs = model(inputs)\n",
        "            optimizer.zero_grad()\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            out = torch.argmax(outputs.detach(),dim=1)\n",
        "            assert out.shape==labels.shape\n",
        "            running_acc += (labels==out).sum().item()\n",
        "        print(f\"Train loss {epoch+1}: {running_loss/len(train_ds)},Train Acc:{running_acc*100/len(train_ds)}%\")\n",
        "\n",
        "\n",
        "%time\n",
        "fast_effective_unlearning(model, forget_classes, retainedSamples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8mZloif9hIS",
        "outputId": "f30cf780-a989-4bef-a6aa-e16098dd6f6d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 6.68 µs\n",
            "Phase 1: Learning Noise for forget classes\n",
            "Learning noise matrices for class= 1\n",
            "Loss: -9.631937980651855\n",
            "Learning noise matrices for class= 3\n",
            "Loss: -4.250012397766113\n",
            "Learning noise matrices for class= 9\n",
            "Loss: -8.33691692352295\n"
          ]
        }
      ]
    }
  ]
}